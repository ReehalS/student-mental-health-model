{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive Bayes : 0.18182\n",
      "Accuracy of Stochastic Gradient Descent : 0.17267\n",
      "Accuracy of KNN : 0.18468\n",
      "Accuracy of Decission trees : 0.16581\n",
      "Accuracy of Random Forest : 0.18811\n",
      "Accuracy of Support Vector Machine : 0.19668\n",
      "Accuracy of Logistic Regression : 0.17667\n",
      "Accuracy of Neural Nets : 0.19611\n"
     ]
    }
   ],
   "source": [
    "# ALL Genres:\n",
    "\n",
    "# Usual Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sklearn\n",
    "\n",
    "import warnings\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# from xgboost import XGBClassifier, XGBRFClassifier\n",
    "# from xgboost import plot_tree, plot_importance\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "le = LabelEncoder()\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# List of genres\n",
    "majors = ['main']\n",
    "# Load the corresponding datasets for the selected genres\n",
    "file_names = [f'../Data/clean_df_{genre}.csv' for genre in majors]\n",
    "data_frames = []\n",
    "\n",
    "for file_name in file_names:\n",
    "    if os.path.exists(file_name):\n",
    "        data_frames.append(pd.read_csv(file_name))\n",
    "    else:\n",
    "        print(f\"Warning: {file_name} not found.\")\n",
    "\n",
    "if not data_frames:\n",
    "    print(f\"No datasets found for {majors}. Exiting.\")\n",
    "    exit\n",
    "\n",
    "\n",
    "# Concatenate all genre datasets into a single DataFrame\n",
    "data = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "# Course,Gender,Sleep_Quality,Physical_Activity,Diet_Quality,Social_Support,Relationship_Status,Substance_Use,Counseling_Service_Use,Family_History,Chronic_Illness,Extracurricular_Involvement,Residence_Type\n",
    "# CGPA, Semester_Credit_Load, Age, Anxiety_Score, Stress_Level, Financial_Stress\n",
    "\n",
    "# Drop specified columns if they exist\n",
    "columns_to_drop = ['Course','Gender','Sleep_Quality','Physical_Activity','Diet_Quality','Social_Support','Relationship_Status','Substance_Use','Counseling_Service_Use','Family_History','Chronic_Illness','Extracurricular_Involvement','Residence_Type']\n",
    "data = data.drop(columns=[col for col in columns_to_drop if col in data.columns])\n",
    "\n",
    "# Fit and transform y_train and y_test\n",
    "\n",
    "y = data['Depression_Score']\n",
    "X = data.drop(columns=['Depression_Score'])\n",
    "# X['Course'] = le.fit_transform(X['Course'])\n",
    "\n",
    "X = X.apply(pd.to_numeric, errors='coerce')\n",
    "X = X.dropna()\n",
    "y = y[X.index]\n",
    "\n",
    "# Normalize X\n",
    "min_max_scaler = MinMaxScaler()\n",
    "np_scaled = min_max_scaler.fit_transform(X)\n",
    "X = pd.DataFrame(np_scaled, columns=X.columns)\n",
    "\n",
    "# Split the data (20-30% testing split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Accuracy of models\n",
    "\n",
    "def model_assess(model, title = \"Default\"):\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    #print(confusion_matrix(y_test, preds))\n",
    "    print('Accuracy of', title, ':', round(accuracy_score(y_test, preds), 5))\n",
    "\n",
    "# Naive Bayes\n",
    "nb = GaussianNB()\n",
    "model_assess(nb, \"Naive Bayes\")\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "sgd = SGDClassifier(max_iter=5000, random_state=0)\n",
    "model_assess(sgd, \"Stochastic Gradient Descent\")\n",
    "\n",
    "# KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=19)\n",
    "model_assess(knn, \"KNN\")\n",
    "\n",
    "# Decission trees\n",
    "tree = DecisionTreeClassifier()\n",
    "model_assess(tree, \"Decission trees\")\n",
    "\n",
    "# Random Forest\n",
    "rforest = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "model_assess(rforest, \"Random Forest\")\n",
    "\n",
    "# Support Vector Machine\n",
    "svm = SVC(decision_function_shape=\"ovo\")\n",
    "model_assess(svm, \"Support Vector Machine\")\n",
    "\n",
    "# Logistic Regression\n",
    "lg = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\n",
    "model_assess(lg, \"Logistic Regression\")\n",
    "\n",
    "# Neural Nets\n",
    "nn = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5000, 10), random_state=1)\n",
    "model_assess(nn, \"Neural Nets\")\n",
    "\n",
    "# Testing more models\n",
    "\n",
    "# def model_assess_encoded(model, X_train, y_train, X_test, y_test, title=\"Default\"):\n",
    "#     # Train model with encoded labels\n",
    "#     model.fit(X_train, y_train)\n",
    "#     preds = model.predict(X_test)\n",
    "#     preds_decoded = le.inverse_transform(preds)\n",
    "#     print(f\"Accuracy of {title}\", round(accuracy_score(y_test, preds), 5))\n",
    "\n",
    "# # Cross Gradient Booster\n",
    "# xgb = XGBClassifier(n_estimators=1000, learning_rate=0.05)\n",
    "# model_assess_encoded(xgb, X_train, y_train, X_test, y_test, \"Extreme Gradient Booster\")\n",
    "\n",
    "# # Cross Gradient Booster (Random Forest)\n",
    "# xgbrf = XGBRFClassifier(objective= 'multi:softmax')\n",
    "# model_assess_encoded(xgbrf, X_train, y_train, X_test, y_test, \"Extreme Gradient Booster (Random Forest)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Stress_Level</th>\n",
       "      <th>Depression_Score</th>\n",
       "      <th>Anxiety_Score</th>\n",
       "      <th>Financial_Stress</th>\n",
       "      <th>Semester_Credit_Load</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>3.56</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>2.44</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>3.74</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>3.40</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>3.35</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  CGPA  Stress_Level  Depression_Score  Anxiety_Score  Financial_Stress  \\\n",
       "0   25  3.56             3                 3              2                 2   \n",
       "1   24  2.44             0                 3              0                 3   \n",
       "2   19  3.74             4                 0              3                 4   \n",
       "3   18  3.40             3                 3              4                 0   \n",
       "4   21  3.35             2                 4              3                 5   \n",
       "\n",
       "   Semester_Credit_Load  \n",
       "0                    17  \n",
       "1                    27  \n",
       "2                    15  \n",
       "3                    23  \n",
       "4                    19  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Feature  Random Forest  Logistic Regression  \\\n",
      "3                          CGPA       0.126577             0.204673   \n",
      "0                           Age       0.095695            -0.121981   \n",
      "17         Semester_Credit_Load       0.094268            -0.190708   \n",
      "5                 Anxiety_Score       0.071369             0.058907   \n",
      "15             Financial_Stress       0.068901             0.190518   \n",
      "4                  Stress_Level       0.064896            -0.032960   \n",
      "1                        Course       0.063016             0.735039   \n",
      "18               Residence_Type       0.044518            -0.047058   \n",
      "16  Extracurricular_Involvement       0.043720            -0.023271   \n",
      "7             Physical_Activity       0.042752             0.032305   \n",
      "10          Relationship_Status       0.041104            -0.141535   \n",
      "8                  Diet_Quality       0.040866            -0.097646   \n",
      "9                Social_Support       0.039824             0.098067   \n",
      "6                 Sleep_Quality       0.038856            -0.131008   \n",
      "12       Counseling_Service_Use       0.037553            -0.013635   \n",
      "2                        Gender       0.028705             0.047856   \n",
      "13               Family_History       0.024818             0.055905   \n",
      "11                Substance_Use       0.023783             0.018117   \n",
      "14              Chronic_Illness       0.008782             0.104828   \n",
      "\n",
      "    Permutation Importance (RF)  \n",
      "3                      0.005374  \n",
      "0                     -0.008005  \n",
      "17                    -0.004117  \n",
      "5                     -0.009834  \n",
      "15                     0.005775  \n",
      "4                     -0.007604  \n",
      "1                      0.021326  \n",
      "18                    -0.003888  \n",
      "16                    -0.002687  \n",
      "7                     -0.010577  \n",
      "10                     0.001887  \n",
      "8                     -0.000915  \n",
      "9                     -0.003602  \n",
      "6                      0.002287  \n",
      "12                    -0.001887  \n",
      "2                      0.002058  \n",
      "13                     0.000858  \n",
      "11                    -0.000629  \n",
      "14                     0.005374  \n",
      "Best Parameters (Random Forest): {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 500}\n",
      "Ensemble Accuracy: 0.20754716981132076\n",
      "Workflow Complete! Track results and improve further.\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "le = LabelEncoder()\n",
    "\n",
    "data = pd.read_csv('../../Data/clean_df_main.csv')\n",
    "\n",
    "# Load Data\n",
    "# Assuming 'data' is your dataset\n",
    "y = data['Depression_Score']\n",
    "X = data.drop(columns=['Depression_Score'])\n",
    "X['Course'] = le.fit_transform(X['Course'])\n",
    "X = X.apply(pd.to_numeric, errors='coerce').dropna()\n",
    "y = y[X.index]\n",
    "\n",
    "# Normalize Features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# Split Dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Feature Importance Analysis\n",
    "feature_importance_results = {}\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_importances = rf.feature_importances_\n",
    "feature_importance_results['Random Forest'] = rf_importances\n",
    "\n",
    "# Logistic Regression\n",
    "lr = LogisticRegression(max_iter=500, random_state=42)\n",
    "lr.fit(X_train, y_train)\n",
    "lr_importances = lr.coef_[0]\n",
    "feature_importance_results['Logistic Regression'] = lr_importances\n",
    "\n",
    "# Permutation Importance for SVM/Neural Networks\n",
    "rf_perm_importance = permutation_importance(rf, X_test, y_test, n_repeats=10, random_state=42)\n",
    "feature_importance_results['Permutation Importance (RF)'] = rf_perm_importance.importances_mean\n",
    "\n",
    "# Combine Feature Importances\n",
    "importance_df = pd.DataFrame({'Feature': X.columns})\n",
    "for model_name, importances in feature_importance_results.items():\n",
    "    importance_df[model_name] = importances\n",
    "print(importance_df.sort_values(by='Random Forest', ascending=False))\n",
    "\n",
    "# Address Class Imbalance with SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Hyperparameter Tuning for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_resampled, y_resampled)\n",
    "print(\"Best Parameters (Random Forest):\", grid_search.best_params_)\n",
    "\n",
    "# Ensemble Learning\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('rf', RandomForestClassifier(**grid_search.best_params_, random_state=42)),\n",
    "    ('xgb', XGBClassifier(random_state=42)),\n",
    "    ('lr', LogisticRegression(random_state=42))\n",
    "], voting='soft')\n",
    "voting_clf.fit(X_train, y_train)\n",
    "ensemble_accuracy = accuracy_score(y_test, voting_clf.predict(X_test))\n",
    "print(\"Ensemble Accuracy:\", ensemble_accuracy)\n",
    "\n",
    "# Evaluate and Iterate\n",
    "print(\"Workflow Complete! Track results and improve further.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Permuted t-SNE for Feature Influence - Sandeep**\n",
    "\n",
    "Run t-SNE on the full dataset and create a baseline plot.\n",
    "\n",
    "Permute (shuffle) each feature individually and re-run t-SNE, comparing the new plot with the baseline.\n",
    "\n",
    "If permuting a feature disrupts the structure or clustering patterns in t-SNE, that feature may be important for defining the dataâ€™s structure. Compile the influential features from this approach and report them as significant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Course: business\n",
      "Feature: Age, Similarity Score: 9.25458384781558\n",
      "Feature: Gender, Similarity Score: 9.120181977379707\n",
      "Feature: CGPA, Similarity Score: 8.313250950680771\n",
      "Feature: Stress_Level, Similarity Score: 8.890500011252907\n",
      "Feature: Anxiety_Score, Similarity Score: 9.118460998879373\n",
      "Feature: Sleep_Quality, Similarity Score: 8.886865259450891\n",
      "Feature: Physical_Activity, Similarity Score: 9.02887041150499\n",
      "Feature: Diet_Quality, Similarity Score: 9.524121152640964\n",
      "Feature: Social_Support, Similarity Score: 8.021417475776385\n",
      "Feature: Relationship_Status, Similarity Score: 8.955750922789857\n",
      "Feature: Substance_Use, Similarity Score: 9.529161262887252\n",
      "Feature: Counseling_Service_Use, Similarity Score: 9.57174681422628\n",
      "Feature: Family_History, Similarity Score: 10.920136059245117\n",
      "Feature: Chronic_Illness, Similarity Score: 12.36255474082217\n",
      "Feature: Financial_Stress, Similarity Score: 8.50165192986926\n",
      "Feature: Extracurricular_Involvement, Similarity Score: 9.28013782329254\n",
      "Feature: Semester_Credit_Load, Similarity Score: 9.042136039619892\n",
      "Feature: Residence_Type, Similarity Score: 8.745595731341533\n",
      "Important features for business: ['Age', 'Gender', 'CGPA', 'Stress_Level', 'Anxiety_Score', 'Sleep_Quality', 'Physical_Activity', 'Diet_Quality', 'Social_Support', 'Relationship_Status', 'Substance_Use', 'Counseling_Service_Use', 'Family_History', 'Chronic_Illness', 'Financial_Stress', 'Extracurricular_Involvement', 'Semester_Credit_Load', 'Residence_Type']\n",
      "Processing Course: compsci\n",
      "Feature: Age, Similarity Score: 9.602099937787862\n",
      "Feature: Gender, Similarity Score: 11.308659468309706\n",
      "Feature: CGPA, Similarity Score: 9.926876267721703\n",
      "Feature: Stress_Level, Similarity Score: 10.69061034291192\n",
      "Feature: Anxiety_Score, Similarity Score: 10.112713857026426\n",
      "Feature: Sleep_Quality, Similarity Score: 11.15321752759239\n",
      "Feature: Physical_Activity, Similarity Score: 10.242065389587252\n",
      "Feature: Diet_Quality, Similarity Score: 9.670717565534588\n",
      "Feature: Social_Support, Similarity Score: 10.452732217280252\n",
      "Feature: Relationship_Status, Similarity Score: 11.183544800265548\n",
      "Feature: Substance_Use, Similarity Score: 11.586053061831002\n",
      "Feature: Counseling_Service_Use, Similarity Score: 10.338526192908475\n",
      "Feature: Family_History, Similarity Score: 13.083013647417806\n",
      "Feature: Chronic_Illness, Similarity Score: 13.960418185478176\n",
      "Feature: Financial_Stress, Similarity Score: 10.45047211372919\n",
      "Feature: Extracurricular_Involvement, Similarity Score: 10.529607547558625\n",
      "Feature: Semester_Credit_Load, Similarity Score: 9.949977871029233\n",
      "Feature: Residence_Type, Similarity Score: 9.697306710573102\n",
      "Important features for compsci: ['Age', 'Gender', 'CGPA', 'Stress_Level', 'Anxiety_Score', 'Sleep_Quality', 'Physical_Activity', 'Diet_Quality', 'Social_Support', 'Relationship_Status', 'Substance_Use', 'Counseling_Service_Use', 'Family_History', 'Chronic_Illness', 'Financial_Stress', 'Extracurricular_Involvement', 'Semester_Credit_Load', 'Residence_Type']\n",
      "Processing Course: engineering\n",
      "Feature: Age, Similarity Score: 10.045206032271102\n",
      "Feature: Gender, Similarity Score: 12.94367951778623\n",
      "Feature: CGPA, Similarity Score: 10.1794563295445\n",
      "Feature: Stress_Level, Similarity Score: 10.644763449623778\n",
      "Feature: Anxiety_Score, Similarity Score: 10.115232569484792\n",
      "Feature: Sleep_Quality, Similarity Score: 10.005324819394449\n",
      "Feature: Physical_Activity, Similarity Score: 10.231347197221378\n",
      "Feature: Diet_Quality, Similarity Score: 9.955161885084278\n",
      "Feature: Social_Support, Similarity Score: 10.269810708548714\n",
      "Feature: Relationship_Status, Similarity Score: 9.820126737086408\n",
      "Feature: Substance_Use, Similarity Score: 13.065041727539485\n",
      "Feature: Counseling_Service_Use, Similarity Score: 9.825711827031007\n",
      "Feature: Family_History, Similarity Score: 13.639805807284842\n",
      "Feature: Chronic_Illness, Similarity Score: 15.052025438513162\n",
      "Feature: Financial_Stress, Similarity Score: 9.94918765113633\n",
      "Feature: Extracurricular_Involvement, Similarity Score: 11.050600645867112\n",
      "Feature: Semester_Credit_Load, Similarity Score: 9.730499372449328\n",
      "Feature: Residence_Type, Similarity Score: 11.2481791122874\n",
      "Important features for engineering: ['Age', 'Gender', 'CGPA', 'Stress_Level', 'Anxiety_Score', 'Sleep_Quality', 'Physical_Activity', 'Diet_Quality', 'Social_Support', 'Relationship_Status', 'Substance_Use', 'Counseling_Service_Use', 'Family_History', 'Chronic_Illness', 'Financial_Stress', 'Extracurricular_Involvement', 'Semester_Credit_Load', 'Residence_Type']\n",
      "Processing Course: law\n",
      "Feature: Age, Similarity Score: 11.731769242919729\n",
      "Feature: Gender, Similarity Score: 12.676433896574066\n",
      "Feature: CGPA, Similarity Score: 11.817737889904008\n",
      "Feature: Stress_Level, Similarity Score: 12.177940163658452\n",
      "Feature: Anxiety_Score, Similarity Score: 13.467191336242784\n",
      "Feature: Sleep_Quality, Similarity Score: 13.121039485103852\n",
      "Feature: Physical_Activity, Similarity Score: 11.451605993349853\n",
      "Feature: Diet_Quality, Similarity Score: 13.012828016817743\n",
      "Feature: Social_Support, Similarity Score: 12.006670123137628\n",
      "Feature: Relationship_Status, Similarity Score: 12.425521768606462\n",
      "Feature: Substance_Use, Similarity Score: 14.538357094083583\n",
      "Feature: Counseling_Service_Use, Similarity Score: 13.078295060151545\n",
      "Feature: Family_History, Similarity Score: 15.685865196312347\n",
      "Feature: Chronic_Illness, Similarity Score: 17.43836100639989\n",
      "Feature: Financial_Stress, Similarity Score: 11.308189130670272\n",
      "Feature: Extracurricular_Involvement, Similarity Score: 10.965266747404305\n",
      "Feature: Semester_Credit_Load, Similarity Score: 12.36042678578038\n",
      "Feature: Residence_Type, Similarity Score: 12.403631124205587\n",
      "Important features for law: ['Age', 'Gender', 'CGPA', 'Stress_Level', 'Anxiety_Score', 'Sleep_Quality', 'Physical_Activity', 'Diet_Quality', 'Social_Support', 'Relationship_Status', 'Substance_Use', 'Counseling_Service_Use', 'Family_History', 'Chronic_Illness', 'Financial_Stress', 'Extracurricular_Involvement', 'Semester_Credit_Load', 'Residence_Type']\n",
      "Processing Course: medical\n",
      "Feature: Age, Similarity Score: 14.863694825555047\n",
      "Feature: Gender, Similarity Score: 15.484753361915857\n",
      "Feature: CGPA, Similarity Score: 14.122416116840222\n",
      "Feature: Stress_Level, Similarity Score: 13.96931824012706\n",
      "Feature: Anxiety_Score, Similarity Score: 13.893265264288663\n",
      "Feature: Sleep_Quality, Similarity Score: 12.826768285299938\n",
      "Feature: Physical_Activity, Similarity Score: 14.135833026461176\n",
      "Feature: Diet_Quality, Similarity Score: 15.753779084198262\n",
      "Feature: Social_Support, Similarity Score: 14.000114506889465\n",
      "Feature: Relationship_Status, Similarity Score: 11.990872154474253\n",
      "Feature: Substance_Use, Similarity Score: 18.980215187716414\n",
      "Feature: Counseling_Service_Use, Similarity Score: 12.716951868434629\n",
      "Feature: Family_History, Similarity Score: 20.252946585493543\n",
      "Feature: Chronic_Illness, Similarity Score: 18.96553025934701\n",
      "Feature: Financial_Stress, Similarity Score: 14.547842395818336\n",
      "Feature: Extracurricular_Involvement, Similarity Score: 14.47201800797269\n",
      "Feature: Semester_Credit_Load, Similarity Score: 15.699724303318838\n",
      "Feature: Residence_Type, Similarity Score: 12.62275841385554\n",
      "Important features for medical: ['Age', 'Gender', 'CGPA', 'Stress_Level', 'Anxiety_Score', 'Sleep_Quality', 'Physical_Activity', 'Diet_Quality', 'Social_Support', 'Relationship_Status', 'Substance_Use', 'Counseling_Service_Use', 'Family_History', 'Chronic_Illness', 'Financial_Stress', 'Extracurricular_Involvement', 'Semester_Credit_Load', 'Residence_Type']\n",
      "Processing Course: others\n",
      "Feature: Age, Similarity Score: 7.548806604381759\n",
      "Feature: Gender, Similarity Score: 10.418036161053243\n",
      "Feature: CGPA, Similarity Score: 7.7767835122334805\n",
      "Feature: Stress_Level, Similarity Score: 8.413712915344796\n",
      "Feature: Anxiety_Score, Similarity Score: 7.781290350115584\n",
      "Feature: Sleep_Quality, Similarity Score: 10.774989384825062\n",
      "Feature: Physical_Activity, Similarity Score: 8.837559341705811\n",
      "Feature: Diet_Quality, Similarity Score: 8.244966730916582\n",
      "Feature: Social_Support, Similarity Score: 7.9210919131077375\n",
      "Feature: Relationship_Status, Similarity Score: 10.036239413662207\n",
      "Feature: Substance_Use, Similarity Score: 9.927138514244028\n",
      "Feature: Counseling_Service_Use, Similarity Score: 10.610966166088948\n",
      "Feature: Family_History, Similarity Score: 9.2148707492408\n",
      "Feature: Chronic_Illness, Similarity Score: 11.766829812226508\n",
      "Feature: Financial_Stress, Similarity Score: 8.662318445189847\n",
      "Feature: Extracurricular_Involvement, Similarity Score: 7.724911553035434\n",
      "Feature: Semester_Credit_Load, Similarity Score: 7.891118322019416\n",
      "Feature: Residence_Type, Similarity Score: 8.25293684607146\n",
      "Important features for others: ['Age', 'Gender', 'CGPA', 'Stress_Level', 'Anxiety_Score', 'Sleep_Quality', 'Physical_Activity', 'Diet_Quality', 'Social_Support', 'Relationship_Status', 'Substance_Use', 'Counseling_Service_Use', 'Family_History', 'Chronic_Illness', 'Financial_Stress', 'Extracurricular_Involvement', 'Semester_Credit_Load', 'Residence_Type']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 18 elements, new values have 6 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 113\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Save the important features data to a CSV file\u001b[39;00m\n\u001b[1;32m    112\u001b[0m important_features_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(course_important_features)\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m--> 113\u001b[0m \u001b[43mimportant_features_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImportant_Features_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcourse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m course \u001b[38;5;129;01min\u001b[39;00m courses]\n\u001b[1;32m    114\u001b[0m important_features_df\u001b[38;5;241m.\u001b[39mto_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(csv_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimportant_features.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Output summary of important features for all courses\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ECS171_ResearchProject/lib/python3.9/site-packages/pandas/core/generic.py:6313\u001b[0m, in \u001b[0;36mNDFrame.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   6311\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   6312\u001b[0m     \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name)\n\u001b[0;32m-> 6313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__setattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m   6315\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32mproperties.pyx:69\u001b[0m, in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ECS171_ResearchProject/lib/python3.9/site-packages/pandas/core/generic.py:814\u001b[0m, in \u001b[0;36mNDFrame._set_axis\u001b[0;34m(self, axis, labels)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;124;03mThis is called from the cython code when we set the `index` attribute\u001b[39;00m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;124;03mdirectly, e.g. `series.index = [1, 2, 3]`.\u001b[39;00m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    813\u001b[0m labels \u001b[38;5;241m=\u001b[39m ensure_index(labels)\n\u001b[0;32m--> 814\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ECS171_ResearchProject/lib/python3.9/site-packages/pandas/core/internals/managers.py:238\u001b[0m, in \u001b[0;36mBaseBlockManager.set_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_axis\u001b[39m(\u001b[38;5;28mself\u001b[39m, axis: AxisInt, new_labels: Index) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;66;03m# Caller is responsible for ensuring we have an Index object.\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_set_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis] \u001b[38;5;241m=\u001b[39m new_labels\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ECS171_ResearchProject/lib/python3.9/site-packages/pandas/core/internals/base.py:98\u001b[0m, in \u001b[0;36mDataManager._validate_set_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m new_len \u001b[38;5;241m!=\u001b[39m old_len:\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength mismatch: Expected axis has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mold_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m elements, new \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m elements\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length mismatch: Expected axis has 18 elements, new values have 6 elements"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from openTSNE import TSNE\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# List of courses and their dataset file paths\n",
    "courses = ['business', 'compsci', 'engineering', 'law', 'medical', 'others']\n",
    "file_paths = [f'../Data/clean_df_{course}.csv' for course in courses]\n",
    "\n",
    "# Create directories for saving plots and CSV\n",
    "output_dir = \"../Output/\"\n",
    "plots_dir = os.path.join(output_dir, \"plots\")\n",
    "csv_dir = os.path.join(output_dir, \"data\")\n",
    "\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "os.makedirs(csv_dir, exist_ok=True)\n",
    "\n",
    "# Function to load and preprocess the dataset\n",
    "def preprocess_data(file_path, target_col, features):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data[features]\n",
    "    y = data[target_col]\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return X_scaled, y\n",
    "\n",
    "# Function to run TSNE and return results\n",
    "def run_tsne(data):\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30, metric=\"euclidean\")\n",
    "    tsne_results = tsne.fit(data)\n",
    "    return tsne_results\n",
    "\n",
    "# Function to plot TSNE results\n",
    "def plot_tsne(tsne_results, y, title, course, feature=None):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=y, cmap=\"viridis\", s=10, alpha=0.7)\n",
    "    plt.colorbar(label=\"Depression_Score\")\n",
    "    plt.title(title)\n",
    "    \n",
    "    # Save plot as PNG\n",
    "    plot_filename = f\"{course}_{feature if feature else 'baseline'}.png\"\n",
    "    plt.savefig(os.path.join(plots_dir, plot_filename))\n",
    "    plt.close()\n",
    "\n",
    "# Feature permutation t-SNE analysis\n",
    "def permute_feature_and_assess(X, y, features, course):\n",
    "    baseline_tsne = run_tsne(X)\n",
    "    plot_tsne(baseline_tsne, y, f\"Baseline t-SNE for {course}\", course)\n",
    "\n",
    "    baseline_distances = euclidean_distances(baseline_tsne)\n",
    "    important_features = []\n",
    "    \n",
    "    for feature in features:\n",
    "        # Permute the feature\n",
    "        X_permuted = X.copy()\n",
    "        X_permuted[:, features.index(feature)] = np.random.permutation(X_permuted[:, features.index(feature)])\n",
    "        \n",
    "        # Compute t-SNE for permuted data\n",
    "        permuted_tsne = run_tsne(X_permuted)\n",
    "        \n",
    "        # Plot and save permuted feature t-SNE\n",
    "        plot_tsne(permuted_tsne, y, f\"t-SNE after permuting {feature} for {course}\", course, feature)\n",
    "        \n",
    "        permuted_distances = euclidean_distances(permuted_tsne)\n",
    "        \n",
    "        # Calculate similarity score (lower score = more disruption)\n",
    "        similarity_score = np.mean(np.abs(baseline_distances - permuted_distances))\n",
    "        \n",
    "        print(f\"Feature: {feature}, Similarity Score: {similarity_score}\")\n",
    "        if similarity_score > 0.1:  # Threshold to determine feature importance (adjustable)\n",
    "            important_features.append(feature)\n",
    "    \n",
    "    return important_features\n",
    "\n",
    "# Columns in the dataset\n",
    "features = [\n",
    "    \"Age\", \"Gender\", \"CGPA\", \"Stress_Level\", \"Anxiety_Score\", \"Sleep_Quality\", \n",
    "    \"Physical_Activity\", \"Diet_Quality\", \"Social_Support\", \"Relationship_Status\", \n",
    "    \"Substance_Use\", \"Counseling_Service_Use\", \"Family_History\", \"Chronic_Illness\", \n",
    "    \"Financial_Stress\", \"Extracurricular_Involvement\", \"Semester_Credit_Load\", \"Residence_Type\"\n",
    "]\n",
    "target_col = 'Depression_Score'\n",
    "\n",
    "# Dictionary to store important features for each course\n",
    "course_important_features = {}\n",
    "\n",
    "# Process each course\n",
    "for course, file_path in zip(courses, file_paths):\n",
    "    print(f\"Processing Course: {course}\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found: {file_path}. Skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # Preprocess data\n",
    "    X, y = preprocess_data(file_path, target_col, features)\n",
    "    \n",
    "    # Run feature permutation analysis\n",
    "    important_features = permute_feature_and_assess(X, y, features, course)\n",
    "    course_important_features[course] = important_features\n",
    "    print(f\"Important features for {course}: {important_features}\")\n",
    "\n",
    "# Save the important features data to a CSV file\n",
    "important_features_df = pd.DataFrame(course_important_features).T\n",
    "important_features_df.columns = [f\"Important_Features_{course}\" for course in courses]\n",
    "important_features_df.to_csv(os.path.join(csv_dir, \"important_features.csv\"))\n",
    "\n",
    "# Output summary of important features for all courses\n",
    "print(\"\\nSummary of important features for all courses:\")\n",
    "print(important_features_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../Data/clean_df_main.csv')\n",
    "\n",
    "columns_to_drop = ['Course','Gender','Sleep_Quality','Physical_Activity','Diet_Quality','Social_Support','Relationship_Status','Substance_Use','Counseling_Service_Use','Family_History','Chronic_Illness','Extracurricular_Involvement','Residence_Type']\n",
    "data = data.drop(columns=[col for col in columns_to_drop if col in data.columns])\n",
    "\n",
    "data.to_csv('../Data/clean_df_main_topSixCols.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive Bayes : 0.75929\n",
      "Accuracy of Stochastic Gradient Descent : 0.75929\n",
      "Accuracy of KNN : 0.75586\n",
      "Accuracy of Decission trees : 0.61635\n",
      "Accuracy of Random Forest : 0.74157\n",
      "Accuracy of Support Vector Machine : 0.75929\n",
      "Accuracy of Logistic Regression : 0.75929\n",
      "Accuracy of Neural Nets : 0.75415\n",
      "Accuracy of Extreme Gradient Booster : 0.71584\n",
      "Accuracy of Extreme Gradient Booster (Random Forest) : 0.757\n"
     ]
    }
   ],
   "source": [
    "# ALL Genres:\n",
    "\n",
    "# Usual Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sklearn\n",
    "\n",
    "import warnings\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from xgboost import plot_tree, plot_importance\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "le = LabelEncoder()\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "depression_threshold = 3\n",
    "\n",
    "# Load the dataset\n",
    "file_name = '../Data/clean_df_.csv'\n",
    "\n",
    "# Concatenate all genre datasets into a single DataFrame\n",
    "data = pd.read_csv(file_name)\n",
    "\n",
    "# Fit and transform y_train and y_test\n",
    "\n",
    "data['Depression_Score'] = data['Depression_Score'].apply(lambda x: 1 if x>depression_threshold else 0)\n",
    "y = data['Depression_Score']\n",
    "\n",
    "X = data.drop(columns=['Depression_Score'])\n",
    "# X['Course'] = le.fit_transform(X['Course'])\n",
    "\n",
    "X = X.apply(pd.to_numeric, errors='coerce')\n",
    "X = X.dropna()\n",
    "y = y[X.index]\n",
    "\n",
    "# Normalize X\n",
    "min_max_scaler = MinMaxScaler()\n",
    "np_scaled = min_max_scaler.fit_transform(X)\n",
    "X = pd.DataFrame(np_scaled, columns=X.columns)\n",
    "\n",
    "# Split the data (20-30% testing split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Accuracy of models\n",
    "\n",
    "def model_assess(model, title = \"Default\"):\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    #print(confusion_matrix(y_test, preds))\n",
    "    print('Accuracy of', title, ':', round(accuracy_score(y_test, preds), 5))\n",
    "\n",
    "# Naive Bayes\n",
    "nb = GaussianNB()\n",
    "model_assess(nb, \"Naive Bayes\")\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "sgd = SGDClassifier(max_iter=5000, random_state=0)\n",
    "model_assess(sgd, \"Stochastic Gradient Descent\")\n",
    "\n",
    "# KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=19)\n",
    "model_assess(knn, \"KNN\")\n",
    "\n",
    "# Decission trees\n",
    "tree = DecisionTreeClassifier()\n",
    "model_assess(tree, \"Decission trees\")\n",
    "\n",
    "# Random Forest\n",
    "rforest = RandomForestClassifier(n_estimators=55, max_features=1, random_state=42)\n",
    "model_assess(rforest, \"Random Forest\")\n",
    "\n",
    "# Support Vector Machine\n",
    "svm = SVC(decision_function_shape=\"ovo\")\n",
    "model_assess(svm, \"Support Vector Machine\")\n",
    "\n",
    "# Logistic Regression\n",
    "lg = LogisticRegression(random_state=42, solver='lbfgs', multi_class='multinomial')\n",
    "model_assess(lg, \"Logistic Regression\")\n",
    "\n",
    "# Neural Nets\n",
    "nn = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5000, 10), random_state=1)\n",
    "model_assess(nn, \"Neural Nets\")\n",
    "\n",
    "# Testing more models\n",
    "\n",
    "# Cross Gradient Booster\n",
    "xgb = XGBClassifier(n_estimators=1000, learning_rate=0.05)\n",
    "model_assess(xgb, \"Extreme Gradient Booster\")\n",
    "\n",
    "# Cross Gradient Booster (Random Forest)\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "xgbrf = XGBRFClassifier(objective= 'multi:softmax', num_class=num_classes)\n",
    "model_assess(xgbrf, \"Extreme Gradient Booster (Random Forest)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive Bayes: 0.85019\n",
      "Accuracy of Stochastic Gradient Descent: 0.88015\n",
      "Accuracy of KNN: 0.88015\n",
      "Accuracy of Decision Trees: 0.73408\n",
      "Accuracy of Random Forest: 0.88015\n",
      "Accuracy of Support Vector Machine: 0.88015\n",
      "Accuracy of Logistic Regression: 0.88015\n",
      "Accuracy of Neural Nets: 0.8015\n",
      "Accuracy of XGBoost: 0.86142\n",
      "Accuracy of XGBRFClassifier: 0.88015\n",
      "Accuracy of Bagging Classifier: 0.88015\n",
      "Accuracy of Extra Trees Classifier: 0.88015\n",
      "Accuracy of Linear Discriminant Analysis: 0.88015\n",
      "Accuracy of Quadratic Discriminant Analysis: 0.8427\n",
      "Accuracy of AdaBoost: 0.86891\n",
      "Accuracy of Voting Classifier: 0.88015\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load and preprocess data\n",
    "depression_threshold = 4\n",
    "file_name = '../Data/clean_df_engineering.csv'\n",
    "data = pd.read_csv(file_name)\n",
    "data.drop(['Course'], axis=1, inplace=True)\n",
    "data['Depression_Score'] = data['Depression_Score'].apply(lambda x: 1 if x > depression_threshold else 0)\n",
    "y = data['Depression_Score']\n",
    "X = data.drop(columns=['Depression_Score'])\n",
    "X = X.apply(pd.to_numeric, errors='coerce').dropna()\n",
    "y = y[X.index]\n",
    "\n",
    "# Normalize features\n",
    "scaler = MinMaxScaler()\n",
    "# X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "accuracies = {}\n",
    "\n",
    "# Model evaluation function\n",
    "def model_assess(model, title=\"Default\"):\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    accuracies[model.__class__.__name__] = accuracy_score(y_test, preds)\n",
    "    print(f'Accuracy of {title}: {round(accuracy_score(y_test, preds), 5)}')\n",
    "\n",
    "# Model definitions\n",
    "models = {\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Stochastic Gradient Descent\": SGDClassifier(max_iter=5000, random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=19),\n",
    "    \"Decision Trees\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=55, max_features=1, random_state=42),\n",
    "    \"Support Vector Machine\": SVC(decision_function_shape=\"ovo\"),\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42, solver='lbfgs', multi_class='multinomial'),\n",
    "    \"Neural Nets\": MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5000, 10), random_state=1),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators=10000, learning_rate=0.03),\n",
    "    \"XGBRFClassifier\": XGBRFClassifier(objective='multi:softmax', num_class=len(np.unique(y_train))),\n",
    "    \"Bagging Classifier\": BaggingClassifier(DecisionTreeClassifier(), n_estimators=50, random_state=42),\n",
    "    \"Extra Trees Classifier\": ExtraTreesClassifier(n_estimators=1000, random_state=42),\n",
    "    \"Linear Discriminant Analysis\": LinearDiscriminantAnalysis(),\n",
    "    \"Quadratic Discriminant Analysis\": QuadraticDiscriminantAnalysis(),\n",
    "    \"AdaBoost\": AdaBoostClassifier(n_estimators=50, random_state=42),\n",
    "    \"Voting Classifier\": VotingClassifier(estimators=[\n",
    "        ('lr', LogisticRegression()),\n",
    "        ('rf', RandomForestClassifier(random_state=42)),\n",
    "        ('svc', SVC(probability=True))\n",
    "    ], voting='soft')\n",
    "}\n",
    "\n",
    "# Evaluate models\n",
    "for title, model in models.items():\n",
    "    model_assess(model, title)\n",
    "\n",
    "# # Keras Neural Network\n",
    "# model = Sequential([\n",
    "#     Dense(128, input_dim=X_train.shape[1], activation='relu'),\n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dense(1, activation='sigmoid')\n",
    "# ])\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "# loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "# print(f'Accuracy of Keras Neural Network: {round(accuracy, 5)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SGDClassifier', 0.8801498127340824),\n",
       " ('KNeighborsClassifier', 0.8801498127340824),\n",
       " ('RandomForestClassifier', 0.8801498127340824),\n",
       " ('SVC', 0.8801498127340824),\n",
       " ('LogisticRegression', 0.8801498127340824),\n",
       " ('XGBRFClassifier', 0.8801498127340824),\n",
       " ('BaggingClassifier', 0.8801498127340824),\n",
       " ('ExtraTreesClassifier', 0.8801498127340824),\n",
       " ('LinearDiscriminantAnalysis', 0.8801498127340824),\n",
       " ('VotingClassifier', 0.8801498127340824),\n",
       " ('AdaBoostClassifier', 0.8689138576779026),\n",
       " ('XGBClassifier', 0.8614232209737828),\n",
       " ('GaussianNB', 0.850187265917603),\n",
       " ('QuadraticDiscriminantAnalysis', 0.8426966292134831),\n",
       " ('MLPClassifier', 0.8014981273408239),\n",
       " ('DecisionTreeClassifier', 0.7340823970037453)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(accuracies.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with 6 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive Bayes: 0.88015\n",
      "Accuracy of Stochastic Gradient Descent: 0.88015\n",
      "Accuracy of KNN: 0.88015\n",
      "Accuracy of Decision Trees: 0.77154\n",
      "Accuracy of Random Forest: 0.88015\n",
      "Accuracy of Support Vector Machine: 0.88015\n",
      "Accuracy of Logistic Regression: 0.88015\n",
      "Accuracy of Neural Nets: 0.80899\n",
      "Accuracy of XGBoost: 0.83521\n",
      "Accuracy of XGBRFClassifier: 0.88015\n",
      "Accuracy of Bagging Classifier: 0.88015\n",
      "Accuracy of Extra Trees Classifier: 0.88015\n",
      "Accuracy of Linear Discriminant Analysis: 0.88015\n",
      "Accuracy of Quadratic Discriminant Analysis: 0.88015\n",
      "Accuracy of AdaBoost: 0.87266\n",
      "Accuracy of Voting Classifier: 0.88015\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "depression_threshold = 4\n",
    "file_name = '../Data/clean_df_engineering.csv'\n",
    "data = pd.read_csv(file_name)\n",
    "data.drop(['Course'], axis=1, inplace=True)\n",
    "data['Depression_Score'] = data['Depression_Score'].apply(lambda x: 1 if x > depression_threshold else 0)\n",
    "\n",
    "features = ['Age', 'CGPA', 'Semester_Credit_Load', 'Anxiety_Score', 'Financial_Stress', 'Stress_Level']\n",
    "\n",
    "y = data['Depression_Score']\n",
    "X = data[features]\n",
    "# X = X.apply(pd.to_numeric, errors='coerce').dropna()\n",
    "# y = y[X.index]\n",
    "\n",
    "# Normalize features\n",
    "scaler = MinMaxScaler()\n",
    "# X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "accuracies = {}\n",
    "\n",
    "# Model evaluation function\n",
    "def model_assess(model, title=\"Default\"):\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    accuracies[model.__class__.__name__] = accuracy_score(y_test, preds)\n",
    "    print(f'Accuracy of {title}: {round(accuracy_score(y_test, preds), 5)}')\n",
    "\n",
    "# Model definitions\n",
    "models = {\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Stochastic Gradient Descent\": SGDClassifier(max_iter=5000, random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=19),\n",
    "    \"Decision Trees\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=55, max_features=1, random_state=42),\n",
    "    \"Support Vector Machine\": SVC(decision_function_shape=\"ovo\"),\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42, solver='lbfgs', multi_class='multinomial'),\n",
    "    \"Neural Nets\": MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5000, 10), random_state=1),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators=10000, learning_rate=0.03),\n",
    "    \"XGBRFClassifier\": XGBRFClassifier(objective='multi:softmax', num_class=len(np.unique(y_train))),\n",
    "    \"Bagging Classifier\": BaggingClassifier(DecisionTreeClassifier(), n_estimators=50, random_state=42),\n",
    "    \"Extra Trees Classifier\": ExtraTreesClassifier(n_estimators=1000, random_state=42),\n",
    "    \"Linear Discriminant Analysis\": LinearDiscriminantAnalysis(),\n",
    "    \"Quadratic Discriminant Analysis\": QuadraticDiscriminantAnalysis(),\n",
    "    \"AdaBoost\": AdaBoostClassifier(n_estimators=50, random_state=42),\n",
    "    \"Voting Classifier\": VotingClassifier(estimators=[\n",
    "        ('lr', LogisticRegression()),\n",
    "        ('rf', RandomForestClassifier(random_state=42)),\n",
    "        ('svc', SVC(probability=True))\n",
    "    ], voting='soft')\n",
    "}\n",
    "\n",
    "# Evaluate models\n",
    "for title, model in models.items():\n",
    "    model_assess(model, title)\n",
    "\n",
    "# # Keras Neural Network\n",
    "# model = Sequential([\n",
    "#     Dense(128, input_dim=X_train.shape[1], activation='relu'),\n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dense(1, activation='sigmoid')\n",
    "# ])\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "# loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "# print(f'Accuracy of Keras Neural Network: {round(accuracy, 5)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('GaussianNB', 0.8801498127340824),\n",
       " ('SGDClassifier', 0.8801498127340824),\n",
       " ('KNeighborsClassifier', 0.8801498127340824),\n",
       " ('RandomForestClassifier', 0.8801498127340824),\n",
       " ('SVC', 0.8801498127340824),\n",
       " ('LogisticRegression', 0.8801498127340824),\n",
       " ('XGBRFClassifier', 0.8801498127340824),\n",
       " ('BaggingClassifier', 0.8801498127340824),\n",
       " ('ExtraTreesClassifier', 0.8801498127340824),\n",
       " ('LinearDiscriminantAnalysis', 0.8801498127340824),\n",
       " ('QuadraticDiscriminantAnalysis', 0.8801498127340824),\n",
       " ('VotingClassifier', 0.8801498127340824),\n",
       " ('AdaBoostClassifier', 0.8726591760299626),\n",
       " ('XGBClassifier', 0.8352059925093633),\n",
       " ('MLPClassifier', 0.8089887640449438),\n",
       " ('DecisionTreeClassifier', 0.7715355805243446)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(accuracies.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with 4 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('GaussianNB', 0.8801498127340824),\n",
       " ('SGDClassifier', 0.8801498127340824),\n",
       " ('KNeighborsClassifier', 0.8801498127340824),\n",
       " ('SVC', 0.8801498127340824),\n",
       " ('LogisticRegression', 0.8801498127340824),\n",
       " ('LinearDiscriminantAnalysis', 0.8801498127340824),\n",
       " ('QuadraticDiscriminantAnalysis', 0.8801498127340824),\n",
       " ('VotingClassifier', 0.8801498127340824),\n",
       " ('RandomForestClassifier', 0.8764044943820225),\n",
       " ('XGBRFClassifier', 0.8764044943820225),\n",
       " ('ExtraTreesClassifier', 0.8764044943820225),\n",
       " ('BaggingClassifier', 0.8726591760299626),\n",
       " ('AdaBoostClassifier', 0.8689138576779026),\n",
       " ('XGBClassifier', 0.8614232209737828),\n",
       " ('MLPClassifier', 0.8239700374531835),\n",
       " ('DecisionTreeClassifier', 0.7565543071161048)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "depression_threshold = 4\n",
    "file_name = '../Data/clean_df_engineering.csv'\n",
    "data = pd.read_csv(file_name)\n",
    "data.drop(['Course'], axis=1, inplace=True)\n",
    "data['Depression_Score'] = data['Depression_Score'].apply(lambda x: 1 if x > depression_threshold else 0)\n",
    "\n",
    "features = ['Age', 'CGPA', 'Semester_Credit_Load', 'Anxiety_Score']\n",
    "\n",
    "y = data['Depression_Score']\n",
    "X = data[features]\n",
    "# X = X.apply(pd.to_numeric, errors='coerce').dropna()\n",
    "# y = y[X.index]\n",
    "\n",
    "# Normalize features\n",
    "scaler = MinMaxScaler()\n",
    "# X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "accuracies = {}\n",
    "\n",
    "# Model evaluation function\n",
    "def model_assess(model, title=\"Default\"):\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    accuracies[model.__class__.__name__] = accuracy_score(y_test, preds)\n",
    "    # print(f'Accuracy of {title}: {round(accuracy_score(y_test, preds), 5)}')\n",
    "\n",
    "# Model definitions\n",
    "models = {\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Stochastic Gradient Descent\": SGDClassifier(max_iter=5000, random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=19),\n",
    "    \"Decision Trees\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=55, max_features=1, random_state=42),\n",
    "    \"Support Vector Machine\": SVC(decision_function_shape=\"ovo\"),\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42, solver='lbfgs', multi_class='multinomial'),\n",
    "    \"Neural Nets\": MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5000, 10), random_state=1),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators=10000, learning_rate=0.03),\n",
    "    \"XGBRFClassifier\": XGBRFClassifier(objective='multi:softmax', num_class=len(np.unique(y_train))),\n",
    "    \"Bagging Classifier\": BaggingClassifier(DecisionTreeClassifier(), n_estimators=50, random_state=42),\n",
    "    \"Extra Trees Classifier\": ExtraTreesClassifier(n_estimators=1000, random_state=42),\n",
    "    \"Linear Discriminant Analysis\": LinearDiscriminantAnalysis(),\n",
    "    \"Quadratic Discriminant Analysis\": QuadraticDiscriminantAnalysis(),\n",
    "    \"AdaBoost\": AdaBoostClassifier(n_estimators=50, random_state=42),\n",
    "    \"Voting Classifier\": VotingClassifier(estimators=[\n",
    "        ('lr', LogisticRegression()),\n",
    "        ('rf', RandomForestClassifier(random_state=42)),\n",
    "        ('svc', SVC(probability=True))\n",
    "    ], voting='soft')\n",
    "}\n",
    "\n",
    "# Evaluate models\n",
    "for title, model in models.items():\n",
    "    model_assess(model, title)\n",
    "\n",
    "# # Keras Neural Network\n",
    "# model = Sequential([\n",
    "#     Dense(128, input_dim=X_train.shape[1], activation='relu'),\n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dense(1, activation='sigmoid')\n",
    "# ])\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "# loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "# print(f'Accuracy of Keras Neural Network: {round(accuracy, 5)}')\n",
    "sorted(accuracies.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
